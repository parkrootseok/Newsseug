{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
      "Installing collected packages: charset-normalizer, requests\n",
      "Successfully installed charset-normalizer-3.3.2 requests-2.32.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.1.1-cp312-cp312-win_amd64.whl.metadata (59 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 4.5/11.5 MB 22.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.5/11.5 MB 14.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.3/11.5 MB 10.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.8/11.5 MB 8.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.6/11.5 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.1/11.5 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.9/11.5 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.4/11.5 MB 5.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.2/11.5 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.7/11.5 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.5 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 4.8 MB/s eta 0:00:00\n",
      "Downloading numpy-2.1.1-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.6 MB 2.1 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.6/12.6 MB 3.8 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.1/12.6 MB 3.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.6/12.6 MB 3.0 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 3.4/12.6 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.9/12.6 MB 3.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 4.7/12.6 MB 3.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 5.5/12.6 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 6.0/12.6 MB 3.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.8/12.6 MB 3.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 7.1/12.6 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.9/12.6 MB 3.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.7/12.6 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.4/12.6 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.0/12.6 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.7/12.6 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.5/12.6 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.8/12.6 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 3.2 MB/s eta 0:00:00\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.1.1 pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.12.3 soupsieve-2.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting boto3Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading boto3-1.35.29-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting botocore<1.36.0,>=1.35.29 (from boto3)\n",
      "  Downloading botocore-1.35.29-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3) (1.0.1)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
      "  Using cached s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from botocore<1.36.0,>=1.35.29->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from botocore<1.36.0,>=1.35.29->boto3) (2.2.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.29->boto3) (1.16.0)\n",
      "Downloading boto3-1.35.29-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.35.29-py3-none-any.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 4.5/12.6 MB 22.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.5/12.6 MB 14.0 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 6.0/12.6 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.8/12.6 MB 8.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.3/12.6 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 8.1/12.6 MB 6.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.7/12.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.4/12.6 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.2/12.6 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.7/12.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.5/12.6 MB 4.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.1/12.6 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 4.7 MB/s eta 0:00:00\n",
      "Using cached s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "Installing collected packages: botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.35.29 botocore-1.35.29 s3transfer-0.10.2\n",
      "Collecting logging\n",
      "  Downloading logging-0.4.9.6.tar.gz (96 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: logging\n",
      "  Building wheel for logging (pyproject.toml): started\n",
      "  Building wheel for logging (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for logging: filename=logging-0.4.9.6-py3-none-any.whl size=29527 sha256=4b02a7285c75f4a8be743a3f86a39535e3b04e0ffca69252e5080e7564184894\n",
      "  Stored in directory: c:\\users\\ssafy\\appdata\\local\\pip\\cache\\wheels\\c2\\46\\de\\9ff6b7043ccdbac300a6f5b61d6220c8f2e9984f7abbf53d96\n",
      "Successfully built logging\n",
      "Installing collected packages: logging\n",
      "Successfully installed logging-0.4.9.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests\n",
    "%pip install pandas\n",
    "%pip install beautifulsoup4\n",
    "%pip install boto3\n",
    "%pip install logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:botocore.hooks:Changing event name from creating-client-class.iot-data to creating-client-class.iot-data-plane\n",
      "DEBUG:botocore.hooks:Changing event name from before-call.apigateway to before-call.api-gateway\n",
      "DEBUG:botocore.hooks:Changing event name from request-created.machinelearning.Predict to request-created.machine-learning.Predict\n",
      "DEBUG:botocore.hooks:Changing event name from before-parameter-build.autoscaling.CreateLaunchConfiguration to before-parameter-build.auto-scaling.CreateLaunchConfiguration\n",
      "DEBUG:botocore.hooks:Changing event name from before-parameter-build.route53 to before-parameter-build.route-53\n",
      "DEBUG:botocore.hooks:Changing event name from request-created.cloudsearchdomain.Search to request-created.cloudsearch-domain.Search\n",
      "DEBUG:botocore.hooks:Changing event name from docs.*.autoscaling.CreateLaunchConfiguration.complete-section to docs.*.auto-scaling.CreateLaunchConfiguration.complete-section\n",
      "DEBUG:botocore.hooks:Changing event name from before-parameter-build.logs.CreateExportTask to before-parameter-build.cloudwatch-logs.CreateExportTask\n",
      "DEBUG:botocore.hooks:Changing event name from docs.*.logs.CreateExportTask.complete-section to docs.*.cloudwatch-logs.CreateExportTask.complete-section\n",
      "DEBUG:botocore.hooks:Changing event name from before-parameter-build.cloudsearchdomain.Search to before-parameter-build.cloudsearch-domain.Search\n",
      "DEBUG:botocore.hooks:Changing event name from docs.*.cloudsearchdomain.Search.complete-section to docs.*.cloudsearch-domain.Search.complete-section\n",
      "DEBUG:botocore.session:Setting config variable for region to 'ap-northeast-2'\n",
      "DEBUG:botocore.loaders:Loading JSON file: c:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\boto3\\data\\s3\\2006-03-01\\resources-1.json\n",
      "DEBUG:botocore.loaders:Loading JSON file: c:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\botocore\\data\\endpoints.json\n",
      "DEBUG:botocore.loaders:Loading JSON file: c:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\botocore\\data\\sdk-default-configuration.json\n",
      "DEBUG:botocore.hooks:Event choose-service-name: calling handler <function handle_service_name_alias at 0x0000019CA351DDA0>\n",
      "DEBUG:botocore.loaders:Loading JSON file: c:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\botocore\\data\\s3\\2006-03-01\\service-2.json.gz\n",
      "DEBUG:botocore.loaders:Loading JSON file: c:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\botocore\\data\\s3\\2006-03-01\\service-2.sdk-extras.json\n",
      "DEBUG:botocore.loaders:Loading JSON file: c:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\botocore\\data\\s3\\2006-03-01\\endpoint-rule-set-1.json.gz\n",
      "DEBUG:botocore.loaders:Loading JSON file: c:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\botocore\\data\\partitions.json\n",
      "DEBUG:botocore.hooks:Event creating-client-class.s3: calling handler <function add_generate_presigned_post at 0x0000019CA347F100>\n",
      "DEBUG:botocore.hooks:Event creating-client-class.s3: calling handler <function lazy_call.<locals>._handler at 0x0000019CA4513560>\n",
      "DEBUG:botocore.hooks:Event creating-client-class.s3: calling handler <function add_generate_presigned_url at 0x0000019CA347EE80>\n",
      "DEBUG:botocore.configprovider:Looking for endpoint for s3 via: environment_service\n",
      "DEBUG:botocore.configprovider:Looking for endpoint for s3 via: environment_global\n",
      "DEBUG:botocore.configprovider:Looking for endpoint for s3 via: config_service\n",
      "DEBUG:botocore.configprovider:Looking for endpoint for s3 via: config_global\n",
      "DEBUG:botocore.configprovider:No configured endpoint found.\n",
      "DEBUG:botocore.endpoint:Setting s3 timeout as (60, 60)\n",
      "DEBUG:botocore.loaders:Loading JSON file: c:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\botocore\\data\\_retry.json\n",
      "DEBUG:botocore.client:Registering retry handlers for service: s3\n",
      "DEBUG:botocore.utils:Registering S3 region redirector handler\n",
      "DEBUG:botocore.utils:Registering S3Express Identity Resolver\n",
      "DEBUG:boto3.resources.factory:Loading s3:s3\n",
      "INFO:root:category (base_url): 경제 (https://www.hankyung.com/economy)\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.hankyung.com:443\n",
      "DEBUG:urllib3.connectionpool:https://www.hankyung.com:443 \"GET /economy HTTP/11\" 200 None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:category (base_url): 정치 (https://www.hankyung.com/politics)\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.hankyung.com:443\n",
      "DEBUG:urllib3.connectionpool:https://www.hankyung.com:443 \"GET /politics HTTP/11\" 200 None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import uuid\n",
    "import boto3\n",
    "import io\n",
    "import logging\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "logging.basicConfig(level='DEBUG')\n",
    "\n",
    "header = {'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
    "                         '(KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'),}\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id='AKIA4SZHNYWTL3EDJVMJ',\n",
    "    aws_secret_access_key='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',\n",
    "    region_name='ap-northeast-2'\n",
    ")\n",
    "\n",
    "s3 = session.resource('s3')\n",
    "bucket_name = 'newsseug-bucket'\n",
    "\n",
    "categories = {\n",
    "    '경제': 'https://www.hankyung.com/economy',\n",
    "    '정치': 'https://www.hankyung.com/politics'\n",
    "}\n",
    "\n",
    "\n",
    "def scrap_article(article_url, category):\n",
    "    response = requests.get(article_url, headers=header)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    try:\n",
    "      \n",
    "        # 해당 페이지 카테고리\n",
    "        page_category = soup.find('a', {'class': 'title'}).get_text(strip=True)\n",
    "        logging.info(f\"page_category: {page_category}\")\n",
    "        \n",
    "        # 찾고있는 카테고리\n",
    "        logging.info(f\"category: {category}\")\n",
    "\n",
    "        category = category.split('_')\n",
    "        \n",
    "        # 페이지 카테고리 != 찾고있는 카테고리 -> 스킵\n",
    "        if page_category not in category:\n",
    "            logging.info(f\"{article_url} 는 해당 카테고리가 아니어서 생략합니다. \")\n",
    "            return None\n",
    "        \n",
    "    except AttributeError:\n",
    "        # 유효하지 않은 url 스킵\n",
    "        logging.error(f\"{article_url} 는 찾을 수 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    # 헤드라인 스크랩\n",
    "    try:\n",
    "        headline = soup.find('h3', {'class': 'next-fit'}).get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        headline = 'N/A'\n",
    "\n",
    "    # 생성일 스크랩\n",
    "    try:\n",
    "        date = soup.find('span', {'class': 'txt-date'})\n",
    "        time = date.find('time').get('datetime')\n",
    "    except AttributeError:\n",
    "        time = 'N/A'\n",
    "\n",
    "    # 본문 스크랩\n",
    "    try:\n",
    "        content = soup.find('p', {'class': 'lead'}).get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        content = 'N/A'\n",
    "\n",
    "    \n",
    "    return {'title': headline, 'time': time, 'content': content}\n",
    "\n",
    "\n",
    "def check_for_new_articles(base_url, category):\n",
    "    \n",
    "    # 결과 받아오기\n",
    "    response = requests.get(base_url, headers=header, allow_redirects=False)\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "print(f\"Location (Redirect URL): {response.headers.get('Location')}\")\n",
    "    \n",
    "    # 받은 결과를 BeautifulSoup로 전달\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 현재 가져온 기사 갯수 초기화\n",
    "    article_count = 0\n",
    "    \n",
    "    # 해당 태그와 일치하는 내용 가져오기\n",
    "    articles = soup.find_all('h3', class_='news-fit')\n",
    "    \n",
    "    # 최신 기사를 가져오기 위하 정렬 수행\n",
    "    articles.reverse()\n",
    "    \n",
    "    # 가져온 card를 탐색\n",
    "    for article in articles:\n",
    "        \n",
    "        # 기사를 3개 추출한 경우 종료\n",
    "        if article_count >= 3:\n",
    "            break\n",
    "\n",
    "        # 기사 url 추출\n",
    "        link_tag = article.find('a', href=True)\n",
    "        if link_tag and 'href' in link_tag.attrs:\n",
    "            article_link = link_tag['href']\n",
    "        else:\n",
    "            logging.warning(\"article_link is None or missing 'href'.\")\n",
    "            continue\n",
    "    \n",
    "        logging.debug(f\"Scraping article: {article_link}\")\n",
    "\n",
    "        # 기사 추출\n",
    "        result = scrap_article(article_link, category)\n",
    "\n",
    "        # 추출된 기사가 있는 경우에만 다음 로직을 수행\n",
    "        if result:\n",
    "            # 추출한 기사를 영상 생성 서버로 전달\n",
    "            print(result)\n",
    "            # 카운팅\n",
    "            article_count += 1\n",
    "    \n",
    "\n",
    "def scrap_all_categories():\n",
    "    \n",
    "    for category, base_url in categories.items():\n",
    "        logging.info(f\"category (base_url): {category} ({base_url})\")\n",
    "        check_for_new_articles(base_url, category)\n",
    "        time.sleep(2)\n",
    "\n",
    "scrap_all_categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❗ AWS Lambda code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import uuid\n",
    "import boto3\n",
    "import io\n",
    "import logging\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': 'joongang-news-crawler from Lambda is do!'\n",
    "    }\n",
    "\n",
    "logging.basicConfig(level='DEBUG')\n",
    "\n",
    "header = {'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
    "                         '(KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'),}\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id='AKIA4SZHNYWTL3EDJVMJ',\n",
    "    aws_secret_access_key='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',\n",
    "    region_name='ap-northeast-2'\n",
    ")\n",
    "\n",
    "s3 = session.resource('s3')\n",
    "bucket_name = 'newsseug-bucket'\n",
    "\n",
    "categories = {\n",
    "    '정치': 'https://www.joongang.co.kr/politics',\n",
    "    '경제': 'https://www.joongang.co.kr/money',\n",
    "    '국제': 'https://www.joongang.co.kr/world',\n",
    "    '사회_사건': 'https://www.joongang.co.kr/society/accident', # 앞 분기 기준\n",
    "    '경제_과학': 'https://www.joongang.co.kr/money/science', # 앞 분기 기준\n",
    "    '사회': 'https://www.joongang.co.kr/society',\n",
    "    '스포츠': 'https://www.joongang.co.kr/sports',\n",
    "}\n",
    "\n",
    "def scrap_article(article_url, category):\n",
    "    response = requests.get(article_url, headers=header)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        # category = soup.find('a', {'class': 'title'}).get_text(strip=True)\n",
    "\n",
    "        page_category = soup.find('a', {'class': 'title'}).get_text(strip=True)\n",
    "        logging.info(f\"page_category: {page_category}\")\n",
    "        \n",
    "        # expected_category = categories.get(category).lower()\n",
    "        logging.info(f\"category: {category}\")\n",
    "\n",
    "        category = category.split('_')\n",
    "        \n",
    "        # 카테고리가 아닌 글은 패스\n",
    "        # if page_category != category:\n",
    "        if page_category not in category:\n",
    "            logging.info(f\"{article_url} 는 해당 카테고리가 아니어서 생략합니다. \")\n",
    "            return None\n",
    "    except AttributeError:\n",
    "        logging.error(f\"{article_url} 는 찾을 수 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    # 헤드라인 스크랩\n",
    "    try:\n",
    "        headline = soup.find('h1', {'class': 'headline'}).get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        headline = 'N/A'\n",
    "\n",
    "    # 생성일 스크랩\n",
    "    try:\n",
    "        date = soup.find('p', {'class': 'date'})\n",
    "        time = date.find('time').get('datetime')\n",
    "    except AttributeError:\n",
    "        time = 'N/A'\n",
    "\n",
    "    # 본문 스크랩\n",
    "    try:\n",
    "        content = soup.find('div', {'class': 'article_body'}).get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        content = 'N/A'\n",
    "\n",
    "    # 기자 이름 스크랩\n",
    "    try:\n",
    "        byline = soup.find('div', {'class': 'byline'})\n",
    "        reporter = byline.find('a').get_text(strip=True).split('\\n')[0]\n",
    "    except AttributeError:\n",
    "        reporter = 'N/A'\n",
    "\n",
    "    return {'title': headline, 'time': time, 'content': content, 'reporter': reporter}\n",
    "\n",
    "def get_last_path_segment(base_url):\n",
    "    try:\n",
    "        url_path = urlparse(base_url).path  # '/society'\n",
    "\n",
    "        path_segments = [segment for segment in url_path.split('/') if segment]\n",
    "\n",
    "        if len(path_segments) > 0:\n",
    "            return path_segments[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No valid path segments found in the URL.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_article_to_s3(article_data, base_url):\n",
    "    file_name = str(uuid.uuid4()) + '.json'\n",
    "\n",
    "    article_df = pd.DataFrame([article_data])\n",
    "\n",
    "    buffer = io.StringIO()\n",
    "    article_df.to_json(buffer, orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    category_slug = get_last_path_segment(base_url)\n",
    "    \n",
    "    try:\n",
    "        # s3.put_object(Bucket=bucket_name, Key=f'content/{file_name}', Body=buffer.getvalue())\n",
    "        s3.Bucket(bucket_name).put_object(Key=f'content/joongang/{category_slug}/{file_name}', Body=buffer.getvalue())\n",
    "        logging.debug(f\"{file_name} 을 {category_slug}/{bucket_name} 에 업로드 하였습니다.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error uploading {file_name}: {e}\")\n",
    "\n",
    "def check_for_new_articles(base_url, category):\n",
    "    response = requests.get(base_url, headers=header)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    article_count = 0\n",
    "    cards = soup.find_all('li', class_='card')\n",
    "    cards.reverse()\n",
    "    for card in cards:\n",
    "        if article_count >= 3:\n",
    "            break\n",
    "\n",
    "        link_tag = card.find('a', href=True)\n",
    "        if link_tag and 'href' in link_tag.attrs:\n",
    "            article_link = link_tag['href']\n",
    "        else:\n",
    "            logging.warning(\"article_link is None or missing 'href'.\")\n",
    "            continue\n",
    "    \n",
    "        logging.debug(f\"Scraping article: {article_link}\")\n",
    "\n",
    "        result = scrap_article(article_link, category)\n",
    "\n",
    "        if result is None:\n",
    "            continue\n",
    "\n",
    "        save_article_to_s3(result, base_url)\n",
    "        article_count += 1\n",
    "\n",
    "def scrap_all_categories():\n",
    "    \n",
    "    for category, base_url in categories.items():\n",
    "        logging.info(f\"category (base_url): {category} ({base_url})\")\n",
    "        check_for_new_articles(base_url, category)\n",
    "        time.sleep(2)\n",
    "\n",
    "scrap_all_categories()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
