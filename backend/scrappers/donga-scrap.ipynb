{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests\n",
    "%pip install pandas\n",
    "%pip install beautifulsoup4\n",
    "%pip install boto3\n",
    "%pip install logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
<<<<<<< HEAD
    "        'body': 'donga-news-crawler from Lambda is do!'\n",
=======
    "        'body': 'joongang-news-crawler from Lambda is do!'\n",
>>>>>>> 3e2fda4d73f14ad4e9dc43befbc104c52c6d1bf9
    "    }\n",
    "\n",
    "logging.basicConfig(level='DEBUG')\n",
    "\n",
    "header = {'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
    "                         '(KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'),}\n",
    "\n",
    "categories = {\n",
    "    '정치': 'https://www.donga.com/news/Politics',\n",
    "    '경제': 'https://www.donga.com/news/Economy',\n",
    "    '국제': 'https://www.donga.com/news/Inter',\n",
    "    '사회_사건': 'https://www.donga.com/news/Society/Event', # 앞 분기 기준\n",
    "    '경제_과학': 'https://www.donga.com/news/It/List', # 앞 분기 기준\n",
    "    '사회': 'https://www.donga.com/news/Society',\n",
    "    '스포츠': 'https://www.donga.com/news/Sports',\n",
    "}\n",
    "\n",
    "result_categories = {\n",
    "    '정치': 'politics',\n",
    "    '경제': 'economy',\n",
    "    '국제': 'world',\n",
    "    '사회_사건': 'accident',\n",
    "    '경제_과학': 'science',\n",
    "    '사회': 'society',\n",
    "    '스포츠': 'sports',\n",
    "}\n",
    "\n",
    "def scrap_article(article_url, category):\n",
    "    response = requests.get(article_url, headers=header)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        page_category = soup.find('ol', class_='breadcrumb').find('li', class_='breadcrumb_item').find('a').get_text(strip=True)\n",
    "        logging.info(f\"page_category: {page_category}\")\n",
    "        \n",
    "        logging.info(f\"category: {category}\")\n",
    "\n",
    "        category_parts = category.split('_')\n",
    "        \n",
    "        # 카테고리가 아닌 글은 패스\n",
    "        if page_category not in category_parts:\n",
    "            logging.info(f\"{article_url} 는 해당 카테고리가 아니어서 생략합니다. \")\n",
    "            return None\n",
    "    except AttributeError:\n",
    "        logging.error(f\"{article_url} 는 찾을 수 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    # 헤드라인 스크랩\n",
    "    try:\n",
    "        title = soup.find('section', {'class': 'head_group'}).find('h1').get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        title = 'N/A'\n",
    "        \n",
    "    try:\n",
    "        date_button = soup.find('button', {'aria-controls': 'dateInfo'})\n",
    "        time_span = date_button.find('span', {'aria-hidden': 'true'})\n",
    "        time = time_span.get_text(strip=True)  # \"2024-09-30 21:44\" 형태\n",
    "        time = time.replace(\" \", \"T\") + \":00+09:00\"  # ISO 8601 형식으로 변환 (예: 2024-09-30T21:44:00+09:00)\n",
    "    except AttributeError:\n",
    "        time = 'N/A'\n",
    "\n",
    "    # 본문 스크랩\n",
    "    try:\n",
    "        # 광고, 이미지 등 제외하고 텍스트만 추출\n",
    "        content_section = soup.find('section', class_='news_view')\n",
    "        for tag in content_section(['figure', 'script', 'div', 'ul']):  # 불필요한 태그 제거\n",
    "            tag.decompose()\n",
    "        content = content_section.get_text(separator=\"\\n\", strip=True)\n",
    "    except AttributeError:\n",
    "        content = 'N/A'\n",
    "\n",
    "    result_category = result_categories.get(category, 'unknown')\n",
    "\n",
    "    return {'title': title, 'time': time, 'content': content, 'source_url': article_url, 'category': result_category}\n",
    "\n",
    "def post_article_to_server(article_data):\n",
    "    \n",
    "    fastapi_url = 'https://your-fastapi-url.com/process_data'\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(fastapi_url, json=article_data, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        logging.info(f'\\'statusCode\\': 200 \\n\\'body\\': \\'요청 성공\\'')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f'\\'statusCode\\': 500 \\n\\'Internal Server Error\\': {e}')\n",
    "\n",
    "def check_for_new_articles(base_url, category):\n",
    "    response = requests.get(base_url, headers=header)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    article_count = 0\n",
    "    cards = soup.find_all('article', class_='news_card')\n",
    "    cards.reverse()\n",
    "    for card in cards:\n",
    "        if article_count >= 3:\n",
    "            break\n",
    "\n",
    "        link_tag = card.find('a', href=True)\n",
    "        if link_tag and 'href' in link_tag.attrs:\n",
    "            article_link = link_tag['href']\n",
    "        else:\n",
    "            logging.warning(\"article_link is None or missing 'href'.\")\n",
    "            continue\n",
    "    \n",
    "        logging.debug(f\"Scraping article: {article_link}\")\n",
    "\n",
    "        result = scrap_article(article_link, category)\n",
    "\n",
    "        if result is None:\n",
    "            continue\n",
<<<<<<< HEAD
    "        \n",
    "        # post_article_to_server(result) # 서버 완성되면 주석을 풀어라...\n",
=======
    "\n",
>>>>>>> 3e2fda4d73f14ad4e9dc43befbc104c52c6d1bf9
    "        print(result)\n",
    "        article_count += 1\n",
    "\n",
    "def scrap_all_categories():\n",
    "    \n",
    "    for category, base_url in categories.items():\n",
    "        logging.info(f\"category (base_url): {category} ({base_url})\")\n",
    "        check_for_new_articles(base_url, category)\n",
    "        time.sleep(2)\n",
    "\n",
    "scrap_all_categories()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
